---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hello! I am a final year Ph.D. student in the Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, advised by [Prof. Patrico A. Vela](https://pvela.gatech.edu/). 

In the summers of both 2024 and 2025, I was a Research Intern at Microsoft Mixed Reality, collaborating with [Dr. Ben Lundell](https://www.linkedin.com/in/benjamin-lundell/) and [Dr. Harpreet Sawhney](https://scholar.google.com/citations?user=73FHLFAAAAAJ&hl=en).
In summer 2023, I was a Applied Scientist intern at Amazon Robotics, working with [Dr. Sisir Karumanchi](https://scholar.google.com/citations?user=3AJYxmsAAAAJ&hl=en) and [Dr. Shuai Han](https://scholar.google.com/citations?user=nGZ45wQAAAAJ&hl=en).
<!-- I am also fortunate to collaborate with [Dr. Benjamin Lundell](https://www.linkedin.com/in/benjamin-lundell) and [Dr. Harpreet Sawhney](https://scholar.google.ca/citations?hl=en&user=73FHLFAAAAAJ&view_op=list_works&sortby=pubdate) from Microsoft. -->

My research interests are in computer vision, language processing, and their integration to advance robotic intelligence. Specifically, my work spans Robotic Grasping (both 6-DoF and planar), Language Command Understanding, and addressing Open World challenges. More recently, I have been developing algorithms that leverage Large Language Models and Vision-Language Models to enable generalizable planning and spatial understanding.

You can find my resume [here](files/resume.pdf) (updated May 2025). 

**Update**: I am actively seeking full-time positions in industry! I am happy to connect regarding potential opportunities!

<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->

# üî• News
<!-- - *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
 - *2025.11*: üìù<a href="https://arxiv.org/abs/2502.03450">SG<sup>2</sup></a> has been accepted to <strong>AAAI 2026</strong>!
 - *2025.05*: üíªExcited to come back to <strong>Microsoft</strong> Mixed Reality team as a research scientist intern.
 - *2025.02*: üìù<a href="https://microsoft.github.io/GASP">GASP</a> has been accepted to <strong>CVPR 2025</strong>!
 - *2024.05*: üíªExcited to join <strong>Microsoft</strong> Mixed Reality team as a research scientist intern.
 <!-- üéâ -->
 - *2023.07*: üìù<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_WDiscOOD_Out-of-Distribution_Detection_via_Whitened_Linear_Discriminant_Analysis_ICCV_2023_paper.html">WDiscOOD</a> has been accepted to <strong>ICCV 2023</strong>!
 - *2023.06*: üìù<a href="https://ieeexplore.ieee.org/document/10342514)">KGNv2</a> has been accepted to <strong>IROS 2023</strong>!
 - *2023.05*: üíªExcited to join <strong>Amazon Robotics</strong> stow perception team as an applied scientist intern.
 - *2023.01*: üìù<a href="https://hychen-naza.github.io/projects/LEAP/index.html">LEAP</a> has been accepted to <strong>ICLR 2023</strong>!
 - *2023.01*: üìù<a href="https://ieeexplore.ieee.org/abstract/document/10161284">KGN</a> has been accepted to <strong>ICRA 2023</strong>!
 - *2021.01*: üìù<a href="https://ieeexplore.ieee.org/abstract/document/9561994">CGNet</a> and <a href="https://ieeexplore.ieee.org/abstract/document/9561964">DLSSNet</a> have been accepted to <strong>ICRA 2021</strong>!

# üìñ Educations
- *2021.01 - 2025 (Expected)*: Ph.D. in Electrical and Computer Engineering, Georgia Tech. Advised by Dr. [Patricio A. Vela](https://pvela.gatech.edu/). Atlanta, GA, United States. 
- *2019.08 - 2020.12*: M.S. in Electrical and Computer Engineering, Georgia Tech. Atlanta, GA, United States.
- *2015.09 - 2019.06*: B.E. in Aerospace Engineering, Beihang University. Beijing, China.

# üíª Industrial Experience 
<div class='industrial-box'>
  <div class="industrial-text">
    <ul>
      <li><strong>2025.05 - 2025.08</strong>: Research Scientist Internship, <a href="https://www.microsoft.com/en-us/research/">Microsoft Mixed Reality</a>.</li>
        <ul>
          <li><strong>Mentor:</strong> Ben Lundell;</li>
          <li><strong>Topic:</strong>Inspecting the vision-action alignment in Vision-Language-Action (VLA) models.</li>
          <li>Redmond, WA, United States.</li>
        </ul>
      <li><strong>2024.05 - 2024.08</strong>: Research Scientist Internship, <a href="https://www.microsoft.com/en-us/research/">Microsoft Mixed Reality</a>.</li>
        <ul>
          <li><strong>Mentor:</strong> Ben Lundell; <strong>Co-Mentor:</strong>Harpreet Sawhney</li>
          <li><strong>Topic:</strong> Reasoning on scene graphs with Large Language Models (LLMs).</li>
          <li>Redmond, WA, United States.</li>
        </ul>
    </ul>
  </div>
  <div class="industrial-image">
    <img src='images/microsoft.png' alt="Microsoft Logo" width="100%">
  </div>
</div>
<div class='industrial-box'>
  <div class="industrial-text">
    <ul>
      <li><strong>2023.05 - 2023.08</strong>: Applied Scientist Internship, <a href="https://www.amazon.science/research-areas/robotics">Amazon Robotics</a>.</li>
        <ul>
          <li><strong>Manager:</strong> Sisir Karumanchi; <strong>Mentor:</strong>Shuai Han</li>
          <li><strong>Topic:</strong> Uncertainty estimation on deep vision models for quantifying the robotic action reliability.</li>
          <li>Seattle, WA, United States.</li>
        </ul>
    </ul>
  </div>
  <div class="industrial-image">
    <img src='images/amazon.png' alt="Microsoft Logo" width="100%">
  </div>
</div>
<!-- - *2024.05 - 2024.08*: Research Scientist Internship, [Microsoft Research](https://www.microsoft.com/en-us/research/). Redmond, WA, United States.
- *2023.05 - 2023.08*: Applied Scientist Internship, [Amazon Robotics](https://www.amazon.science/research-areas/robotics). Seattle, WA, United States. -->

# üìù Publications 
(\* denotes equal contribution)

<!-- RwR -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2026 (Oral)</div><img src='images/RwR_multiagent.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**SG<sup>2</sup>: Schema-Guided Scene-Graph Reasoning based on Multi-Agent Large Language Model System**](https://arxiv.org/abs/2502.03450)

**Yiye Chen**, Harpreet Sawhney, Nicholas Gyde, Yanan Jian, Jack Saunders, Patricio A. Vela, Benjamin Lundell

[Code(Coming Soon)]() | [Project(Coming Soon)]()
<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->
- A schema-guided multiagent LLMs framework for iterative reasoning and planning on scene graphs.
</div>
</div>

<!-- WDiscOOD -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/ICCV2023_WDiscOOD.png' alt="sym" width="85%"></div></div>
<div class='paper-box-text' markdown="1">

[**WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant Analysis**](https://openaccess.thecvf.com/content/ICCV2023/html/Chen_WDiscOOD_Out-of-Distribution_Detection_via_Whitened_Linear_Discriminant_Analysis_ICCV_2023_paper.html)

**Yiye Chen**, Yunzhi Lin, Ruinian Xu, Patricio A. Vela

[Code](https://github.com/ivalab/WDiscOOD) | [Poster](files/poster_WDiscOOD.pdf)
<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->
- A visual representation analysis approach to identify when a deep learning model doesn't know in the open-world setting.
- Showing effectiveness in various vision backbones, including ResNet, Vision Transformer, and CLIP vision encoder.
</div>
</div>

<!-- LEAP -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/ICLR2023_LEAP_teaser.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Planning with Language Models through Iterative Energy Minimization**](https://openreview.net/forum?id=cVFD6qE8gnY)

Hongyi Chen\*, Yilun Du\*, **Yiye Chen\***, Patricio A. Vela, Joshua B. Tenenbaum

[Project](https://hychen-naza.github.io/projects/LEAP/index.html) | [Code](https://github.com/ivalab/WDiscOOD)
<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->
- An energy-based learning and interative sampling method for action sequence planning with Transformer model.
</div>
</div>

<!-- KGNv2 -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IROS 2023</div><img src='images/graphAbs_KGNv2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Synthesis on RGB-D input**](https://ieeexplore.ieee.org/document/10342514)

**Yiye Chen**; Ruinian Xu; Yunzhi Lin; Hongyi Chen; Patricio A. Vela

[Code](https://github.com/ivalab/KGN) | [Presentation](https://youtu.be/gK30iMHPr4I) | [Poster](files/poster_KGNv2.pdf) | [Supplementary](https://youtu.be/XDpdTdHo5eU)
<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->
- Enhances Keypoint-GraspNet (see below) by addressing scale-related issues, where scale refers to the distance of a pose towards the single-view camera.
</div>
</div>

<!-- KGN -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2023</div><img src='images/ICRA23_KGN.png' alt="sym" width="80%"></div></div>
<div class='paper-box-text' markdown="1">

[**Keypoint-GraspNet: Keypoint-based 6-DoF Grasp Generation from the Monocular RGB-D input**](https://ieeexplore.ieee.org/abstract/document/10161284)

**Yiye Chen**; Yunzhi Lin; Ruinian Xu; Patricio A. Vela

[Code](https://github.com/ivalab/KGN) | [Presentation](https://youtu.be/oNHZ05kHBgI) | [Poster](files/poster_KGN.pdf) | [Supplementary](https://youtu.be/cT9lvTajRdA)
<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->
- A keypoint-based approach for generating 6-DoF grasp poses from single-view RGB-D input.
</div>
</div>

<!-- CGNet -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2021</div><img src='images/ICRA21_CGNet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**A Joint Network for Grasp Detection Conditioned on Natural Language Commands**](https://ieeexplore.ieee.org/abstract/document/9561994)

**Yiye Chen**; Ruinian Xu; Yunzhi Lin; Patricio A. Vela

[Presentation](https://youtu.be/tridtJes7uI) | [Supplementary](https://youtu.be/cpUbk5JBNvY?si=ygHC_R8E-HnS-PcL)
<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->
- A language-conditioned robotic grasping method by fusing the visual and language embeddings.
</div>
</div>

<!-- Papers in a simple list without pictures  -->
- [GASP: Gaussian Avatars with Synthetic Priors](https://microsoft.github.io/GASP/), Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrusaitis, **Yiye Chen**, Darren Cosker, Virginia Estellers, Nicholas Gyd√©, Vinay Namboodiri, Benjamin Lundell, **CVPR 2025**.
- [Simultaneous Multi-Level Descriptor Learning and Semantic Segmentation for Domain-Specific Relocalization](https://ieeexplore.ieee.org/abstract/document/9561964), Xiaolong Wu*, **Yiye Chen\***, C√©dric Pradalier, Patricio A. Vela, **ICRA 2021**.

# üìú Academic Services
- **Conference Reviewer**: IROS‚Äô23-24, ICRA‚Äô24, CVPR‚Äô24-25, ICLR‚Äô25
- **Journal Reviewer**: The International Journal of Robotics Research (IJRR), IEEE Robotics and Automation Letters (RA-L), IEEE Transactions on Industrial Electronics (TIE)

<!-- # üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->